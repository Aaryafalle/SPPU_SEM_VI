{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28773090-048d-4707-8b86-c9d9a9fa6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f938dfb0-d314-4e77-a505-c958edd0cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aarya\n",
      "[nltk_data]     falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Aarya\n",
      "[nltk_data]     falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aarya\n",
      "[nltk_data]     falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aarya\n",
      "[nltk_data]     falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aarya falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Aarya falle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3540642-4479-4a0a-8482-510c7492815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) helps computers understand human language. It is widely used in sentiment analysis, chatbots, and text summarization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b182d95-9ede-4402-bac0-54681cb264ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) helps computers understand human language.', 'It is widely used in sentiment analysis, chatbots, and text summarization.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d52835-db02-44ea-96cd-20a59c23e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'helps', 'computers', 'understand', 'human', 'language', '.', 'It', 'is', 'widely', 'used', 'in', 'sentiment', 'analysis', ',', 'chatbots', ',', 'and', 'text', 'summarization', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4a4065-fd9c-479f-b90b-d667318d135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'again', 'did', 'own', 'here', \"shouldn't\", 'to', 'for', 'through', 'from', 'yourself', 'no', 'can', 'how', 'doing', \"won't\", 'when', 'but', 'more', 'm', 'each', 'was', 'y', \"he'll\", 'just', \"i'm\", 'further', 'who', 'yourselves', 'during', 'its', 'same', 'you', \"we'd\", 'theirs', \"we'll\", \"hasn't\", 'only', 'all', 'over', 'their', \"wouldn't\", 'ourselves', \"she's\", 'needn', \"he'd\", \"should've\", \"aren't\", 'so', \"you'll\", 'other', 'into', 'there', 'few', \"hadn't\", 'herself', 'why', \"they've\", 'out', 'they', \"didn't\", 'do', 'd', 'below', 'on', 'with', 'and', 'my', 's', 'before', 'of', 'very', 'been', 'doesn', \"don't\", 'our', 'until', 'has', 'him', 'this', 'were', \"you're\", 'where', \"i'd\", 'what', 'which', 'don', \"wasn't\", 'too', \"he's\", 'in', \"we've\", \"it'll\", 'should', 'myself', \"you've\", 'an', 'by', \"they'll\", 'those', 'mightn', \"mustn't\", 're', 'while', 'between', 'not', 'the', 'because', 'some', \"i'll\", 'me', \"weren't\", 'had', 'didn', 'will', 'won', 'having', \"it's\", 'itself', 'haven', 'his', 'against', \"mightn't\", 'off', \"they're\", 'isn', \"needn't\", 'your', \"they'd\", 'are', 'themselves', 'whom', 'hadn', 'shan', 'such', 'about', 'he', 'am', 'above', \"haven't\", \"isn't\", 'these', 'she', \"that'll\", \"couldn't\", 'does', 'down', 'himself', 'have', 'most', 'couldn', 'that', 'them', \"i've\", 'is', 'once', 'her', \"it'd\", 'yours', 'under', 'i', \"she'd\", 'be', \"you'd\", 'wasn', 't', 'hasn', 'shouldn', 'it', \"we're\", 'ours', 'then', 'both', \"doesn't\", 'up', 'if', 'll', 'now', 'o', 'a', 'weren', \"she'll\", 'mustn', 'or', 'wouldn', 'ma', 'nor', 'ain', 'hers', 'being', \"shan't\", 've', 'we', 'than', 'after', 'at', 'as', 'any', 'aren'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9befc7-32f3-4e6d-ac75-ca2b6aef4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filtered Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "text=\"How to remove stop words with NLTK library in Python7\"\n",
    "text= re.sub('[^a-zA-Z]', ' ', text)\n",
    "tokens= word_tokenize(text.lower())\n",
    "filtered_text= []\n",
    "for w in tokens:\n",
    "  if w not in stop_words:\n",
    "    filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\", tokens)\n",
    "print(\"Filtered Sentence:\", filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7227b12e-ae69-4933-a611-c9cb495f79d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait : wait\n",
      "waiting : wait\n",
      "waited : wait\n",
      "waits : wait\n",
      "Original Sentence: ['wait', 'waiting', 'waited', 'waits']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = ['wait', 'waiting', 'waited', 'waits']\n",
    "for w in stemmed_words:\n",
    "  print(w, \":\", ps.stem(w))\n",
    "print(\"Original Sentence:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74296d3c-d1a6-4281-a636-572f50fbd352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cies is cies\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text= \"studies studying cies cry\"\n",
    "tokenization= nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "  print(\"Lemma for {} is {}\".format(w, lemmatizer.lemmatize(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2eec68-79cc-4bb9-8884-6d7c89e41f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fits', 'NNS')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "data = 'The pink sweater fits her perfectly'\n",
    "words = nltk.word_tokenize(data)\n",
    "for word in words:\n",
    "  print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b34c66b-00a0-41c8-978d-ae1f74be4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph= \"\"\"India is a vast country with second highest populati\n",
    " on in the world. It is a country\n",
    " with diverse cultures, traditions and beliefs. People in India cel\n",
    " ebrate unity in diversity.\n",
    " Festivals like Diwali, Holi, Navratri, Ramzan, Christmas etc. are \n",
    "celebrated by people across India \n",
    "and create a sense of brotherhood and cultural unity. Each festiva\n",
    " l has its religious and cultural importance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365bca4c-a87d-44f1-83d3-371664a9de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india vast country second highest populati world', 'country diverse culture tradition belief', 'people india cel ebrate unity diversity', 'festival like diwali holi navratri ramzan christmas etc', 'celebrated people across india create sense brotherhood cultural unity', 'festiva l religious cultural importance']\n"
     ]
    }
   ],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  review = [wn.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178ea1a9-8cbc-408c-ba6a-940c1f536d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.33061545 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40318254\n",
      "  0.         0.         0.27912828 0.         0.         0.\n",
      "  0.40318254 0.         0.         0.40318254 0.         0.\n",
      "  0.         0.40318254 0.40318254]\n",
      " [0.         0.46262479 0.         0.         0.         0.\n",
      "  0.37935895 0.         0.         0.46262479 0.46262479 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.46262479\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.45529187 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.45529187\n",
      "  0.         0.45529187 0.         0.         0.         0.\n",
      "  0.         0.         0.31520422 0.         0.         0.37334585\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37334585 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339 0.         0.35355339 0.\n",
      "  0.35355339 0.         0.         0.35355339 0.35355339 0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.36523197 0.         0.36523197 0.         0.36523197 0.\n",
      "  0.         0.36523197 0.29949544 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25285463 0.         0.         0.29949544\n",
      "  0.         0.         0.         0.         0.36523197 0.\n",
      "  0.29949544 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.42790272 0.         0.         0.\n",
      "  0.         0.         0.         0.52182349 0.         0.\n",
      "  0.         0.52182349 0.         0.         0.         0.\n",
      "  0.         0.         0.52182349 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8694a45f-4aaf-4223-b4a4-a709a9d93e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
